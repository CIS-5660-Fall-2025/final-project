
#include "application.h"
#include <iostream>
#include <vector>
#include <assert.h>
#include "webgpu_utils.h"

using namespace wgpu;
using namespace std;

// Almost identical to inspect adapter; I think the device is like what we use/our interface and the adapter is reality
void inspectDevice(WGPUDevice device) {
    std::vector<WGPUFeatureName> features;
    size_t featureCount = wgpuDeviceEnumerateFeatures(device, nullptr);
    features.resize(featureCount);
    wgpuDeviceEnumerateFeatures(device, features.data());

    std::cout << "Device features:" << std::endl;
    std::cout << std::hex;
    for (auto f : features) {
        std::cout << " - 0x" << f << std::endl;
    }
    std::cout << std::dec;

    WGPUSupportedLimits limits = {};
    limits.nextInChain = nullptr;

#ifdef WEBGPU_BACKEND_DAWN
    bool success = wgpuDeviceGetLimits(device, &limits) == WGPUStatus_Success;
#else
    bool success = wgpuDeviceGetLimits(device, &limits);
#endif

    if (success) {
        std::cout << "Device limits:" << std::endl;
        std::cout << " - maxTextureDimension1D: " << limits.limits.maxTextureDimension1D << std::endl;
        std::cout << " - maxTextureDimension2D: " << limits.limits.maxTextureDimension2D << std::endl;
        std::cout << " - maxTextureDimension3D: " << limits.limits.maxTextureDimension3D << std::endl;
        std::cout << " - maxTextureArrayLayers: " << limits.limits.maxTextureArrayLayers << std::endl;
        // [...] Extra device limits
    }
}

void inspectAdapter(WGPUAdapter adapter) {
    // Limits
#ifndef __EMSCRIPTEN__
    WGPUSupportedLimits supportedLimits = {};
    supportedLimits.nextInChain = nullptr;

#ifdef WEBGPU_BACKEND_DAWN // Dawn has a WGPUStatus return
    bool success = wgpuAdapterGetLimits(adapter, &supportedLimits) == WGPUStatus_Success;
#else
    bool success = wgpuAdapterGetLimits(adapter, &supportedLimits);
#endif

    if (success) {
        std::cout << "Adapter limits:" << std::endl;
        std::cout << " - maxTextureDimension1D: " << supportedLimits.limits.maxTextureDimension1D << std::endl;
        std::cout << " - maxTextureDimension2D: " << supportedLimits.limits.maxTextureDimension2D << std::endl;
        std::cout << " - maxTextureDimension3D: " << supportedLimits.limits.maxTextureDimension3D << std::endl;
        std::cout << " - maxTextureArrayLayers: " << supportedLimits.limits.maxTextureArrayLayers << std::endl;
    }

#endif // NOT __EMSCRIPTEN__

    // Features
    std::vector<WGPUFeatureName> features;
    size_t featureCount = wgpuAdapterEnumerateFeatures(adapter, nullptr);
    features.resize(featureCount);
    wgpuAdapterEnumerateFeatures(adapter, features.data());

    std::cout << "Adapter features:" << std::endl;
    std::cout << std::hex; // Enable hex mode
    for (WGPUFeatureName &f : features) {
        std::cout << " - 0x" << f << std::endl;
    }
    std::cout << std::dec; // Return to decimal mode
}

void commandBufferTest(WGPUQueue &queue, WGPUDevice &device) {
    WGPUCommandEncoderDescriptor encoderDesc = {};
    encoderDesc.nextInChain = nullptr;
    encoderDesc.label = "Command encoder/creator";
    WGPUCommandEncoder encoder = wgpuDeviceCreateCommandEncoder(device, &encoderDesc);

    // Create a command that does a nothing task
    wgpuCommandEncoderInsertDebugMarker(encoder, "task 1");
    wgpuCommandEncoderInsertDebugMarker(encoder, "task 2");

    // Generate Command Buffer
    WGPUCommandBufferDescriptor cmdBufferDescriptor = {};
    cmdBufferDescriptor.nextInChain = nullptr;
    cmdBufferDescriptor.label = "Command buffer";
    WGPUCommandBuffer cmd = wgpuCommandEncoderFinish(encoder, &cmdBufferDescriptor);
    wgpuCommandEncoderRelease(encoder);

    // With the command buffer, we can submit command queue
    std::cout << "Submitting command..." << std::endl;
    wgpuQueueSubmit(queue, 1, &cmd);
    wgpuCommandBufferRelease(cmd);
    std::cout << "Command submitted" << std::endl;

    for (int i = 0 ; i < 5 ; ++i) {
        std::cout << "Tick/Poll device..." << std::endl;
    #if defined(WEBGPU_BACKEND_DAWN)
        wgpuDeviceTick(device);
    #elif defined(WEBGPU_BACKEND_WGPU)
        wgpuDevicePoll(device, false, nullptr);
    #elif defined(WEBGPU_BACKEND_EMSCRIPTEN)
        emscripten_sleep(100);
    #endif
    }
}

WGPUAdapter requestAdapterSync(WGPUInstance instance, WGPURequestAdapterOptions const* options) {
    // A simple structure holding the local information shared with the
    // onAdapterRequestEnded callback.
    struct UserData {
        WGPUAdapter adapter = nullptr;
        bool requestEnded = false;
    };
    UserData userData;

    // Callback called by wgpuInstanceRequestAdapter when the request returns
    // This is a C++ lambda function, but could be any function defined in the
    // global scope. It must be non-capturing (the brackets [] are empty) so
    // that it behaves like a regular C function pointer, which is what
    // wgpuInstanceRequestAdapter expects (WebGPU being a C API). The workaround
    // is to convey what we want to capture through the pUserData pointer,
    // provided as the last argument of wgpuInstanceRequestAdapter and received
    // by the callback as its last argument.
    auto onAdapterRequestEnded = [](WGPURequestAdapterStatus status, WGPUAdapter adapter, char const* message, void* pUserData) {
        UserData& userData = *reinterpret_cast<UserData*>(pUserData);
        if (status == WGPURequestAdapterStatus_Success) {
            userData.adapter = adapter;
        }
        else {
            std::cout << "Could not get WebGPU adapter: " << message << std::endl;
        }
        userData.requestEnded = true;
        };

    // Call to the WebGPU request adapter procedure
    wgpuInstanceRequestAdapter(
        instance /* equivalent of navigator.gpu */,
        options,
        onAdapterRequestEnded,
        (void*)&userData
    );

    // We wait until userData.requestEnded gets true, will only need delay in Emscripten
#ifdef __EMSCRIPTEN
    while (!userData.requestEnded) {
        emscripten_sleep(100);
    }
#endif

    assert(userData.requestEnded);
    std::cout << "Adapter request ended" << std::endl;

    return userData.adapter;
}

/**
 * Utility function to get a WebGPU device, so that
 *     WGPUDevice device = requestDeviceSync(adapter, options);
 * is roughly equivalent to
 *     const device = await adapter.requestDevice(descriptor);
 * It is very similar to requestAdapter
 */
WGPUDevice requestDeviceSync(WGPUAdapter adapter, WGPUDeviceDescriptor const* descriptor) {
    struct UserData {
        WGPUDevice device = nullptr;
        bool requestEnded = false;
    };
    UserData userData;

    auto onDeviceRequestEnded = [](WGPURequestDeviceStatus status, WGPUDevice device, char const* message, void* pUserData) {
        UserData& userData = *reinterpret_cast<UserData*>(pUserData);
        if (status == WGPURequestDeviceStatus_Success) {
            userData.device = device;
        }
        else {
            std::cout << "Could not get WebGPU device: " << message << std::endl;
        }
        userData.requestEnded = true;
        };

    wgpuAdapterRequestDevice(
        adapter,
        descriptor,
        onDeviceRequestEnded,
        (void*)&userData
    );

#ifdef __EMSCRIPTEN__
    while (!userData.requestEnded) {
        emscripten_sleep(100);
    }
#endif // __EMSCRIPTEN__

    assert(userData.requestEnded);

    return userData.device;
}

bool Application::Initialize() {
    // Get instance
    WGPUInstanceDescriptor desc = {};
    desc.nextInChain = nullptr;

    // For immediate error callback
#ifdef WEBGPU_BACKEND_DAWN
    // Make sure the uncaptured error callback is called as soon as an error
    // occurs rather than at the next call to "wgpuDeviceTick".
    WGPUDawnTogglesDescriptor toggles;
    toggles.chain.next = nullptr;
    toggles.chain.sType = WGPUSType_DawnTogglesDescriptor;
    toggles.disabledToggleCount = 0;
    toggles.enabledToggleCount = 1;
    const char* toggleName = "enable_immediate_error_handling";
    toggles.enabledToggles = &toggleName;

    desc.nextInChain = &toggles.chain;
#endif // WEBGPU_BACKEND_DAWN

    // For some reason, Emscripten doesn't want a descriptor
#ifdef WEBGPU_BACKEND_EMSCRIPTEN
    WGPUInstance instance = wgpuCreateInstance(nullptr);
#else
    WGPUInstance instance = wgpuCreateInstance(&desc);
#endif
    if (instance == nullptr) {
        std::cout << "Could not initialize WebGPU!" << std::endl;
        return false;
    }
    std::cout << "WGPU instance: " << instance << std::endl;

    // Create GLFW Window
    if(!glfwInit()) {
        std::cerr << "Couldn't initialize GLFW!!!" << std::endl;
        return false;
    }

    glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API); // Tell GLFW not to think about API since it wouldn't know WebGPU anyways
    glfwWindowHint(GLFW_RESIZABLE, GLFW_FALSE); // Disallow resizing since it causes crash for now
    window = glfwCreateWindow(640, 480, "Learn WebGPU!!", nullptr, nullptr);
    if(!window) {
        std::cerr << "Couldn't open GLFW window!!" << std::endl;
        glfwTerminate();
        return false;
    }

    // Create surface
    surface = glfwGetWGPUSurface(instance, window);

    // Get adapter
    std::cout << "Requesting Device Adapter to determine capabilities..." << std::endl;
    WGPURequestAdapterOptions adapterOpts = {};
    adapterOpts.nextInChain = nullptr;
    adapterOpts.compatibleSurface = surface;
    WGPUAdapter adapter = requestAdapterSync(instance, &adapterOpts);
    wgpuInstanceRelease(instance); // Don't need instance now that we have adapter
    inspectAdapter(adapter);

    // Get device
    std::cout << "Requesting device..." << std::endl;
    WGPUDeviceDescriptor deviceDesc = {};
    deviceDesc.nextInChain = nullptr;
    deviceDesc.label = "My Device"; // anything works here, that's your call
    deviceDesc.requiredFeatureCount = 0; // we do not require any specific feature
    deviceDesc.defaultQueue.nextInChain = nullptr;
    deviceDesc.defaultQueue.label = "The default queue";
    // A function that is invoked whenever the device stops being available.
    deviceDesc.deviceLostCallback = [](WGPUDeviceLostReason reason, char const* message, void* /* pUserData */) {
        std::cout << "Device lost: reason " << reason;
        if (message) std::cout << " (" << message << ")";
        std::cout << std::endl;
        };
    RequiredLimits limits = GetRequiredLimits(adapter);
    deviceDesc.requiredLimits = nullptr; //&limits; // Require at least these capabilities
    device = requestDeviceSync(adapter, &deviceDesc);
    std::cout << "Got device: " << device << std::endl;
    auto onDeviceError = [](WGPUErrorType type, char const* message, void* /* pUserData */) {
        // Putting breakpoints in this error callback is useful..
        // But in Dawn, the callback will be called when the device 'ticks', less informative
        // So we can use an instance toggle above to force it to immediately call this callback on error
        std::cout << "Uncaptured device error: type " << type;
        if (message) std::cout << " (" << message << ")";
        std::cout << std::endl;
        };
    wgpuDeviceSetUncapturedErrorCallback(device, onDeviceError, nullptr /* pUserData */);
    inspectDevice(device);

    // Queue and Command Buffer
    queue = wgpuDeviceGetQueue(device);
    auto onQueueWorkDone = [](WGPUQueueWorkDoneStatus status, void *) {
        std::cout << "Queued work finished with status: " << status << std::endl;
        };
    wgpuQueueOnSubmittedWorkDone(queue, onQueueWorkDone, nullptr /* pUserData */);

    // Configure surface
    WGPUSurfaceConfiguration config = {};
    config.nextInChain = nullptr;

    config.width = 640;
    config.height = 480;
    surfaceFormat = wgpuSurfaceGetPreferredFormat(surface, adapter);
    config.format = surfaceFormat; // RGBA and channel size chosen by adapter
    config.viewFormatCount = 0;
    config.viewFormats = nullptr;
    config.usage = WGPUTextureUsage_RenderAttachment; // Has to know what it'll be used for
    config.device = device;
    config.presentMode = WGPUPresentMode_Fifo; // How do we do the swap chain?  We have a regular queue here instead of just 2 that we swap
    config.alphaMode = WGPUCompositeAlphaMode_Auto; // How are textures composited onto OS window, could be used for transparent windows

    wgpuSurfaceConfigure(surface, &config);

    wgpuAdapterRelease(adapter);

    InitializeRenderPipeline();

    // [...] Create a first buffer
    BufferDescriptor bufferDesc;
    bufferDesc.label = "GPU Side Data Buffer";
    // CopyDst is copying to GPU, CopySrc is copying to CPU
    bufferDesc.usage = BufferUsage::CopyDst | BufferUsage::CopySrc;
    bufferDesc.size = 16;
    bufferDesc.mappedAtCreation = false; // Buffer also stored on CPU and synced with GPU
    Buffer buf1 = device.createBuffer(bufferDesc);
    // [...] Create a second buffer
    // Reuse Desc
    bufferDesc.label = "Output buffer";
    bufferDesc.usage = BufferUsage::CopyDst | BufferUsage::MapRead;
    Buffer buf2 = device.createBuffer(bufferDesc);
    // [...] Write input data
    std::vector<uint8_t> numbers(16); // 16 1-byte integers, 16 bytes
    for(uint8_t i=0; i<16; ++i) numbers[i] = i;
    queue.writeBuffer(buf1, 0, numbers.data(), numbers.size()); // Write into buf1
    // When this func returns, writing may not be complete but we'll have already created a CPU-side copy
    //   of the numbers data so we can free up the memory from the address we passed
    // Commands in the queue after writeBuffer op won't execute before data transfer finishes
    // [...] Encode and submit the buffer to buffer copy
    CommandEncoder encoder = device.createCommandEncoder(Default);
    encoder.copyBufferToBuffer(buf1, 0, buf2, 0, 16); // the 0 after each buf is the byte offset
    CommandBuffer cmd = encoder.finish(Default);
    encoder.release();
    queue.submit(1, &cmd);
    cmd.release();
    // [...] Read buffer data back
    
    auto onBuf2Mapped = [](WGPUBufferMapAsyncStatus status, void *userData) {
        auto *context = static_cast<std::pair<Buffer, bool>*>(userData);
        context->second = true;
        std::cout << "Buffer 2 mapped with status" << status << std::endl;
        if(status != BufferMapAsyncStatus::Success) return;

        uint8_t *bufferData = (uint8_t*) context->first.getConstMappedRange(0, 16);

        std::cout << "bufferData = [";
        for (int i = 0; i < 16; ++i) {
            if (i > 0) std::cout << ", ";
            std::cout << (int)bufferData[i];
        }
        std::cout << "]" << std::endl;

        context->first.unmap();
    };
    std::pair<Buffer, bool> context = {buf2, false};
    wgpuBufferMapAsync(buf2, MapMode::Read, 0, 16, onBuf2Mapped, &context);
    while(!context.second) {
        wgpuPollEvents(device, true);
    }

    // [...] Release buffers
    buf1.release(); buf2.release();

    InitializeBuffers();

    return true;
}

void Application::Terminate() {
    vertexBuffer.release();
    indexBuffer.release();

    pipeline.release();

    wgpuSurfaceUnconfigure(surface);
    wgpuSurfaceRelease(surface);
    
    glfwDestroyWindow(window);
    glfwTerminate();
    
    wgpuQueueRelease(queue);
    wgpuDeviceRelease(device);
}

void Application::InitializeRenderPipeline() {
    const char* shaderSource = R"(
struct VertexInput {
	@location(0) position: vec2f,
	@location(1) color: vec3f
};

struct VertexOutput {
	@builtin(position) position: vec4f,
	@location(0) color: vec3f
};

@vertex
fn vs_main(vIn: VertexInput) -> VertexOutput {
	var out: VertexOutput;
	out.position = vec4f(vIn.position, 0.0, 1.0);
	out.color = vIn.color;
	return out;
}

@fragment
fn fs_main(in: VertexOutput) -> @location(0) vec4f {
	return vec4(in.color, 1.0);
}
)";

    RenderPipelineDescriptor pipelineDesc;

    // [...] Create Shader Module
    ShaderModuleDescriptor shaderDesc;
    #ifdef WEBGPU_BACKEND_WGPU
    shaderDesc.hintCount = 0;
    shaderDesc.hints = nullptr;
    #endif
    ShaderModuleWGSLDescriptor shaderCodeDesc;
    shaderCodeDesc.chain.next = nullptr;
    shaderCodeDesc.chain.sType = SType::ShaderModuleWGSLDescriptor; // It'll use this for casting
    shaderCodeDesc.code = shaderSource;
    shaderDesc.nextInChain = &shaderCodeDesc.chain; // Can later become a WGSLDescriptor again using the sType
    ShaderModule shaderModule = device.createShaderModule(shaderDesc);


    // [...] Describe vertex pipeline state
        // [...] Describe vertex buffers
        pipelineDesc.vertex.bufferCount = 0;
        pipelineDesc.vertex.buffers = nullptr;
        // [...] Describe vertex shader
        pipelineDesc.vertex.module = shaderModule;
        pipelineDesc.vertex.entryPoint = "vs_main";
        pipelineDesc.vertex.constantCount = 0;
        pipelineDesc.vertex.constants = nullptr;
    // [...] Describe primitive pipeline state
    pipelineDesc.primitive.topology = PrimitiveTopology::TriangleList; // Interpret as triangles
    pipelineDesc.primitive.stripIndexFormat = IndexFormat::Undefined;
    pipelineDesc.primitive.frontFace = FrontFace::CCW; // What counts as front face?
    pipelineDesc.primitive.cullMode = CullMode::None; // Well ^ don't matter for now cuz we're not culling
    // [...] Describe fragment pipeline state
    FragmentState fragmentState;
    fragmentState.module = shaderModule;
    fragmentState.entryPoint = "fs_main";
    fragmentState.constantCount = 0;
    fragmentState.constants = nullptr;
    // [...] Configure blending stage here!
    BlendState blendState;
    // [...] Configure Color Blending which takes form 'col = SourceFactor * src (VariableOperation) DstFactor * dst
    // Src is what we're drawing and Dst is what already exists so Src is on top of Dst
    blendState.color.srcFactor = BlendFactor::SrcAlpha;
    blendState.color.dstFactor = BlendFactor::OneMinusSrcAlpha;
    blendState.color.operation = BlendOperation::Add;
    // [...] Configure Alpha Blending, we'll just make it always 1 for now
    blendState.alpha.srcFactor = BlendFactor::Zero;
    blendState.alpha.dstFactor = BlendFactor::One;
    blendState.alpha.operation = BlendOperation::Add;

    ColorTargetState colorTarget;
    colorTarget.format = surfaceFormat;
    colorTarget.blend = &blendState;
    colorTarget.writeMask = ColorWriteMask::All; // Could write to a subset of color channels

    // Only 1 color attachment so only 1 target color
    fragmentState.targetCount = 1;
    fragmentState.targets = &colorTarget;

    pipelineDesc.fragment = &fragmentState; // Nullable since fragment state is optional
    // [...] Describe stencil/depth pipeline state
    pipelineDesc.depthStencil = nullptr; // Configure ZBuffer test
    // [...] Describe multi-sampling state
    // You can have multiple fragments per pixel and avg the result into a pixel
    pipelineDesc.multisample.count = 1; // But we won't do multisampling
    pipelineDesc.multisample.mask = ~0u; // All bits on
    pipelineDesc.multisample.alphaToCoverageEnabled = false; // Irrelevant for now
    // [...] Describe pipeline layout
    pipelineDesc.layout = nullptr; // Would be important for accessing input and output resources (buffers/textures)
        // nullptr asks backend to figure out layout


    // [...] Describe Vertex Layout
    VertexBufferLayout vertexBufferLayout;

    vector<VertexAttribute> vertexAttribs(2);

    vertexAttribs[0].shaderLocation = 0;
    vertexAttribs[0].format = VertexFormat::Float32x2;
    vertexAttribs[0].offset = 0;

    vertexAttribs[1].shaderLocation = 1;
    vertexAttribs[1].format = VertexFormat::Float32x3;
    vertexAttribs[1].offset = 2 * sizeof(float);
    

    vertexBufferLayout.attributeCount = 2;
    vertexBufferLayout.attributes = vertexAttribs.data();

    vertexBufferLayout.arrayStride = 5 * sizeof(float);
    vertexBufferLayout.stepMode = VertexStepMode::Vertex; // Each new val is a new vertex

    pipelineDesc.vertex.bufferCount = 1;
    pipelineDesc.vertex.buffers = &vertexBufferLayout;
    
    pipeline = device.createRenderPipeline(pipelineDesc);

    shaderModule.release();

}

void Application::MainLoop() {
    glfwPollEvents(); // Process input events

    auto [surfaceTexture, targetView] = GetNextSurfaceViewData();
    if(!targetView) return;

    // Create command encoder to create draw call command
    CommandEncoderDescriptor encoderDesc = {};
    encoderDesc.label = "Command encoder";
    CommandEncoder encoder = device.createCommandEncoder(encoderDesc);

    // Create render pass color attachment
    RenderPassDescriptor renderPassDesc = {};
    RenderPassColorAttachment renderPassColorAttachment = {}; // Only 1 color attach for now
    renderPassColorAttachment.view = targetView;
    renderPassColorAttachment.resolveTarget = nullptr;
    renderPassColorAttachment.loadOp = WGPULoadOp_Clear; // What to do before executing render pass
    renderPassColorAttachment.storeOp = WGPUStoreOp_Store; // What to do after executing render pass
    renderPassColorAttachment.clearValue = WGPUColor{0.1, 1.0, 0.5, 1.0 };
    #ifndef WEBGPU_BACKEND_WGPU
    renderPassColorAttachment.depthSlice = WGPU_DEPTH_SLICE_UNDEFINED; // No depth buffer
    #endif

    renderPassDesc.depthStencilAttachment = nullptr;
    renderPassDesc.timestampWrites = nullptr;
    renderPassDesc.colorAttachmentCount = 1;
    renderPassDesc.colorAttachments = &renderPassColorAttachment; // Array

    RenderPassEncoder renderPass = encoder.beginRenderPass(renderPassDesc);

    // What to draw here
    renderPass.setPipeline(pipeline);
    renderPass.setVertexBuffer(0, vertexBuffer, 0, vertexBuffer.getSize()); // Could change geo here
    renderPass.setIndexBuffer(indexBuffer, IndexFormat::Uint32, 0, indexBuffer.getSize());
    renderPass.drawIndexed(indexCount, 1, 0, 0, 0);

    renderPass.end();
    renderPass.release();

    // Create draw command
    CommandBufferDescriptor cmdBufferDescriptor = {};
    cmdBufferDescriptor.label = "Command buffer";
    CommandBuffer command = encoder.finish(cmdBufferDescriptor);
    encoder.release();

    // Queue draw command
    queue.submit(1, &command);
    command.release();

    // At end of frame
    targetView.release();
    #ifndef __EMSCRIPTEN__ // We use a different way with Emscripten
    surface.present();
    #endif
}

bool Application::IsRunning() {
    return !glfwWindowShouldClose(window);
}

std::pair<SurfaceTexture, TextureView> Application::GetNextSurfaceViewData() {
    // Get next surface texture
    // Surface texture contains actual texture as well as additional info (status)
    WGPUSurfaceTexture surfaceTexture;
    wgpuSurfaceGetCurrentTexture(surface, &surfaceTexture);
    if(surfaceTexture.status != WGPUSurfaceGetCurrentTextureStatus_Success) {
        return {surfaceTexture, nullptr};
    }

    // Create surface texture view
    // Texture view could represent a sub-part of the texture or it in a different format..
    // For now we're using the given boiler plate
    WGPUTextureViewDescriptor viewDescriptor;
    viewDescriptor.nextInChain = nullptr;
    viewDescriptor.label = "Surface texture view";
    viewDescriptor.format = wgpuTextureGetFormat(surfaceTexture.texture);
    viewDescriptor.dimension = WGPUTextureViewDimension_2D;
    viewDescriptor.baseMipLevel = 0;
    viewDescriptor.mipLevelCount = 1;
    viewDescriptor.baseArrayLayer = 0;
    viewDescriptor.arrayLayerCount = 1;
    viewDescriptor.aspect = WGPUTextureAspect_All;
    WGPUTextureView targetView = wgpuTextureCreateView(surfaceTexture.texture, &viewDescriptor);

    // Release texture
    #ifndef WEBGPU_BACKEND_WGPU
    // We no longer need the texture, only its view which contains its own reference to texture
    // with wgpu-native, surface textures must be release after the call to wgpuSurfacePresent
    wgpuTextureRelease(surfaceTexture.texture);
    #endif // WEBGPU_BACKEND_WGPU

    return {surfaceTexture, targetView};
}

RequiredLimits Application::GetRequiredLimits(Adapter adapter) const {
    SupportedLimits supportedLimits;
    adapter.getLimits(&supportedLimits);

    RequiredLimits requiredLimits = Default;
    requiredLimits.limits.minStorageBufferOffsetAlignment = 32;
    requiredLimits.limits.maxVertexAttributes = 2; // Require a max of 1
    requiredLimits.limits.maxVertexBuffers = 1;
    requiredLimits.limits.maxBufferSize = 6 * 5 * sizeof(float);
    requiredLimits.limits.maxVertexBufferArrayStride = 5 * sizeof(float);
    requiredLimits.limits.maxInterStageShaderComponents = 3;
    // We can only draw 2 triangles with these limits..

    return requiredLimits;
}

void Application::InitializeBuffers() {
    vector<float> positions = {
        -0.5f, -0.5f,  1.0f, 0.0f, 0.0f,
        0.5f, -0.5f,  0.0f, 1.0f, 0.0f,
        0.0f, 0.5f,  0.0f, 0.0f, 1.0f,
        
        -0.9f, -0.9f,  1.0f, 0.0f, 0.0f,
        -0.7f, -0.9f,  0.0f, 1.0f, 0.0f,
        -0.85f,-0.7f,  0.0f, 0.0f, 1.0f
    };
    vertexCount = static_cast<uint32_t>(positions.size())/5;

    // indices must be uint16_t or uint32_t
    vector<uint32_t> indices = {
        0, 1, 2,
        3, 4, 5,
        0, 1, 5
    };
    indexCount = static_cast<uint32_t>(indices.size());

    // Vertex Buffer
    BufferDescriptor bufferDesc;
    bufferDesc.size = positions.size() * sizeof(float);
    bufferDesc.usage = BufferUsage::CopyDst | BufferUsage::Vertex;
    bufferDesc.mappedAtCreation = false;
    vertexBuffer = device.createBuffer(bufferDesc);
    queue.writeBuffer(vertexBuffer, 0, positions.data(), bufferDesc.size);

    // Index Buffer
    BufferDescriptor indexBufferDesc;
    indexBufferDesc.size = indices.size() * sizeof(uint32_t);
    indexBufferDesc.usage = BufferUsage::CopyDst | BufferUsage::Index;
    indexBufferDesc.mappedAtCreation = false;
    indexBuffer = device.createBuffer(indexBufferDesc);
    queue.writeBuffer(indexBuffer, 0, indices.data(), indexBufferDesc.size);
    
    // !!! Buffers must be multiple of 4 bytes
    // So make sure your data AND buffer is a good size
    // Could be arbitrary data just avoid trash data
}